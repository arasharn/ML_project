{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFU2M7aWg2Mr",
        "outputId": "b5376719-6918-48fa-dc81-7607593400ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t_lNPst00P8n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "device = 'cuda'#'cpu'#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-uIYGykB6vw"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EZ1BSiRiB6cw"
      },
      "outputs": [],
      "source": [
        "def pacejka(x, B, C, D, E):\n",
        "    if len(x.size()) == 3:\n",
        "        x = torch.squeeze(x, 1)\n",
        "        # expand parameters from [bs] to [bs, 100]\n",
        "        # so that batch can be handled without running loops\n",
        "        B = torch.reshape(B, (-1, 1)).expand(-1, x.size(1))\n",
        "        C = torch.reshape(C, (-1, 1)).expand(-1, x.size(1))\n",
        "        D = torch.reshape(D, (-1, 1)).expand(-1, x.size(1))\n",
        "        E = torch.reshape(E, (-1, 1)).expand(-1, x.size(1))\n",
        "    # print(B.size())\n",
        "    # print(x.size())\n",
        "    y = D * torch.sin(C * torch.arctan(B * x * (1 - E) + E * torch.arctan(B * x)))\n",
        "    return y\n",
        "\n",
        "def magic_formula_loss(x, real_bcde, predicted_bcde):\n",
        "    # print(real_bcde.shape)\n",
        "    B_min, B_max = 4, 12\n",
        "    C_min, C_max = 1, 2\n",
        "    D_min, D_max = 0.1, 1.9\n",
        "    E_min, E_max = -10, 1\n",
        "\n",
        "    # real_bcde[:, :, 0] = real_bcde[:, :, 0]*(B_max - B_min) + B_min\n",
        "    # real_bcde[:, :, 1] = real_bcde[:, :, 1]*(C_max - C_min) + C_min\n",
        "    # real_bcde[:, :, 2] = real_bcde[:, :, 2]*(D_max - D_min) + D_min\n",
        "    # real_bcde[:, :, 3] = real_bcde[:, :, 3]*(E_max - E_min) + E_min\n",
        "\n",
        "    # predicted_bcde[:, 0] = predicted_bcde[:, 0]*(B_max - B_min) + B_min\n",
        "    # predicted_bcde[:, 1] = predicted_bcde[:, 1]*(C_max - C_min) + C_min\n",
        "    # predicted_bcde[:, 2] = predicted_bcde[:, 2]*(D_max - D_min) + D_min\n",
        "    # predicted_bcde[:, 3] = predicted_bcde[:, 3]*(E_max - E_min) + E_min\n",
        "\n",
        "\n",
        "    real_y = pacejka(x, real_bcde[:, :, 0], real_bcde[:, :, 1], real_bcde[:, :, 2], real_bcde[:, :, 3])\n",
        "    predicted_y = pacejka(x, predicted_bcde[:, 0], predicted_bcde[:, 1], predicted_bcde[:, 2], predicted_bcde[:, 3])\n",
        "    loss = nn.MSELoss()(real_y, predicted_y)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def range_condition_loss(B, C, D, E):\n",
        "    B_min, B_max = 4, 12\n",
        "    C_min, C_max = 1, 2\n",
        "    D_min, D_max = 0.1, 1.9\n",
        "    E_min, E_max = -10, 1\n",
        "\n",
        "    # B = B*(B_max - B_min) + B_min\n",
        "    # C = C*(C_max - C_min) + C_min\n",
        "    # D = D*(D_max - D_min) + D_min\n",
        "    # E = E*(E_max - E_min) + E_min\n",
        "\n",
        "    loss = F.relu(B_min - B) + F.relu(B - B_max)\n",
        "    loss += F.relu(C_min - C) + F.relu(C - C_max)\n",
        "    loss += F.relu(D_min - D) + F.relu(D - D_max)\n",
        "    loss += F.relu(E_min - E) + F.relu(E - E_max)\n",
        "    loss = loss.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def slope_loss(B, C, D, predB, predC, predD):\n",
        "    B_min, B_max = 4, 12\n",
        "    C_min, C_max = 1, 2\n",
        "    D_min, D_max = 0.1, 1.9\n",
        "\n",
        "    # B = B*(B_max - B_min) + B_min\n",
        "    # C = C*(C_max - C_min) + C_min\n",
        "    # D = D*(D_max - D_min) + D_min\n",
        "\n",
        "    # predB = predB*(B_max - B_min) + B_min\n",
        "    # predC = predC*(C_max - C_min) + C_min\n",
        "    # predD = predD*(D_max - D_min) + D_min\n",
        "\n",
        "    original_slope = B * C * D\n",
        "    predicted_slope = predB * predC * predD\n",
        "    loss = nn.MSELoss()(original_slope, predicted_slope)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def peak_loss(x_real_peak, y_real_peak, x_pred_peak, y_pred_peak):\n",
        "    loss = nn.MSELoss()(x_real_peak, x_pred_peak) + nn.MSELoss()(y_real_peak, y_pred_peak)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqTQr1gZz65l"
      },
      "source": [
        "## Generating data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cZSs2zi3PUF"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EanMtcz61KJG"
      },
      "outputs": [],
      "source": [
        "def generate_data(x_grid_size=100, resampled_len=10, batch_size=100, noise_sd=0.04):\n",
        "    x = torch.linspace(0, 1, x_grid_size)\n",
        "\n",
        "    B_list = torch.linspace(4, 12, 9)\n",
        "    C_list = torch.linspace(1, 2, 11)\n",
        "    D_list = torch.linspace(0.1, 1.9, 21)\n",
        "    E_list = torch.linspace(-10, 1, 11)\n",
        "\n",
        "    resampled_mu_noisy = torch.zeros(size=(batch_size, resampled_len))\n",
        "    resampled_x = torch.zeros(size=(batch_size, resampled_len))\n",
        "    parameters = torch.zeros(size=(batch_size, 4))\n",
        "    clean_mu = torch.zeros(size=(batch_size, x_grid_size))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        B_idx = torch.randint(high=B_list.size(0), size=(1,))\n",
        "        C_idx = torch.randint(high=C_list.size(0), size=(1,))\n",
        "        D_idx = torch.randint(high=D_list.size(0), size=(1,))\n",
        "        E_idx = torch.randint(high=E_list.size(0), size=(1,))\n",
        "\n",
        "        B = B_list[B_idx]\n",
        "        C = C_list[C_idx]\n",
        "        D = D_list[D_idx]\n",
        "        E = E_list[E_idx]\n",
        "\n",
        "        mu_clean = pacejka(x, B, C, D, E)\n",
        "\n",
        "        mu_noisy = noise_maker(mu_clean, noise_sd)\n",
        "        idx_resample = torch.randint(high=x_grid_size, size=(resampled_len,))\n",
        "        resampled_mu_noisy[i, :] = mu_noisy[idx_resample]\n",
        "        resampled_x[i, :] = x[idx_resample]\n",
        "        parameters[i, 0] = B\n",
        "        parameters[i, 1] = C\n",
        "        parameters[i, 2] = D\n",
        "        parameters[i, 3] = E\n",
        "        clean_mu[i, :] = mu_clean\n",
        "\n",
        "    return resampled_mu_noisy, resampled_x, parameters, clean_mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9j56OUT01-Cu"
      },
      "outputs": [],
      "source": [
        "def noise_maker(data, sd):\n",
        "    low = -sd\n",
        "    high = +sd\n",
        "    noise = (high - low) * torch.rand(size=data.size())\n",
        "    noisy_data = data + noise\n",
        "    return noisy_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNi-_fqC3SnD"
      },
      "source": [
        "### Generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWzrCGyj1vh1"
      },
      "outputs": [],
      "source": [
        "resampled_mu_noisy, resampled_x, parameters, clean_mu = generate_data(x_grid_size=100, resampled_len=10, batch_size=25000, noise_sd=0.04)\n",
        "resampled_mu_noisy = resampled_mu_noisy.unsqueeze(1)\n",
        "resampled_x = resampled_x.unsqueeze(1)\n",
        "parameters = parameters.unsqueeze(1)\n",
        "clean_mu = clean_mu.unsqueeze(1)\n",
        "resampled_mu_noisy = resampled_mu_noisy.to(device)\n",
        "resampled_x = resampled_x.to(device)\n",
        "scaled_parameters = torch.zeros_like(parameters)\n",
        "scaled_parameters[:,:,0] = (parameters[:,:,0]-parameters[:,:,0].min())/(parameters[:,:,0].max()-parameters[:,:,0].min())\n",
        "scaled_parameters[:,:,1] = (parameters[:,:,1]-parameters[:,:,1].min())/(parameters[:,:,1].max()-parameters[:,:,1].min())\n",
        "scaled_parameters[:,:,2] = (parameters[:,:,2]-parameters[:,:,2].min())/(parameters[:,:,2].max()-parameters[:,:,2].min())\n",
        "scaled_parameters[:,:,3] = (parameters[:,:,3]-parameters[:,:,3].min())/(parameters[:,:,3].max()-parameters[:,:,3].min())\n",
        "parameters = parameters.to(device)\n",
        "scaled_parameters = scaled_parameters.to(device)\n",
        "clean_mu = clean_mu.to(device)\n",
        "print(\"resampled_mu_noisy: \", resampled_mu_noisy.shape)\n",
        "print(\"resampled_x: \", resampled_x.shape)\n",
        "print(\"parameters: \", parameters.shape)\n",
        "print(\"clean_mu: \", clean_mu.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmMXdgtb3yVg"
      },
      "source": [
        "#### Data Shuffling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ukzIEr10qm"
      },
      "outputs": [],
      "source": [
        "idx_train, idx_val = torch.utils.data.random_split(torch.arange(0, 25000), [20000, 5000])\n",
        "idx_train = idx_train.indices\n",
        "idx_val = idx_val.indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuDfl95f7OzM"
      },
      "source": [
        "## CWGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "32DGrqyX3Dhe"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.conv1 = torch.nn.Conv1d(1, 4, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool = torch.nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "    #self.conv2 = torch.nn.Conv1d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "    self.fc1 = torch.nn.Linear(48, 24)\n",
        "    self.fc2 = torch.nn.Linear(24, 12)\n",
        "    self.fc3 = torch.nn.Linear(12, 10)\n",
        "    self.fc4 = torch.nn.Linear(10, 12)\n",
        "    self.fc5 = torch.nn.Linear(12, 24)\n",
        "    self.fc6 = torch.nn.Linear(24, 4)\n",
        "  def forward(self, y_sample, x_sample, z):\n",
        "    # print(x_sample.shape)\n",
        "    # print(y_sample.shape)\n",
        "    # print(z.shape)\n",
        "    conditional_tensor = torch.cat((y_sample, x_sample,  z), dim=2)\n",
        "    # print(conditional_tensor.shape)\n",
        "    x = torch.relu(self.conv1(conditional_tensor))\n",
        "    x = self.pool(x)\n",
        "    #print(x.shape)\n",
        "    x = x.view(-1, x.size(-2) * x.size(-1))\n",
        "    #print(x.shape)\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = torch.relu(self.fc3(x))\n",
        "    x = torch.relu(self.fc4(x))\n",
        "    x = torch.relu(self.fc5(x))\n",
        "    x = self.fc6(x)\n",
        "    # x[:,0:3] = torch.abs(x[:,0:3])\n",
        "    # x[:,1] = torch.exp(x[:,1])\n",
        "    # x[:,2] = torch.exp(x[:,2])\n",
        "    # print(x[:,0:3].shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytEIh9PJ-0gP"
      },
      "outputs": [],
      "source": [
        "# G = Generator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ3zL2doF-GA"
      },
      "outputs": [],
      "source": [
        "# x = torch.linspace(0, 1, 100)\n",
        "# G(resampled_mu_noisy[idx_train, :, :], resampled_x[idx_train, :, :], torch.randn_like(clean_mu)[idx_train, :, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waUAVWLq4n3y"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.fc1 = nn.Linear(24, 32)  # Assuming mu and sigma are 2-dimensional\n",
        "    self.fc2 = nn.Linear(32, 16)\n",
        "    self.fc3 = nn.Linear(16, 8)\n",
        "    self.fc4 = nn.Linear(8, 4)\n",
        "    self.fc5 = nn.Linear(4, 1)\n",
        "\n",
        "  def forward(self, y_sample, x_sample, y_hat):\n",
        "    y = torch.cat((y_sample, x_sample, y_hat), dim = 2)\n",
        "    # print(\"Disc>>>>>\")\n",
        "    # print(y.shape)\n",
        "    critic = torch.relu(self.fc1(y))\n",
        "    critic = torch.relu(self.fc2(critic))\n",
        "    critic = torch.relu(self.fc3(critic))\n",
        "    critic = torch.relu(self.fc4(critic))\n",
        "    return self.fc5(critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgVkcp2L6NXi"
      },
      "outputs": [],
      "source": [
        "# D = Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtXv3rVpJmYx"
      },
      "outputs": [],
      "source": [
        "# D(resampled_mu_noisy[idx_train, :, :], resampled_x[idx_train, :, :], torch.randn_like(clean_mu)[idx_train, :, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3usu41bgbx78"
      },
      "outputs": [],
      "source": [
        "def calculate_gradient_penalty(y_sample_true, x_sample_true, y_true,\n",
        "        y_sample_fake, x_sample_fake, y_hat_fake,\n",
        "                               D):\n",
        "  # print(\"gp>>>>>\")\n",
        "  # print(y_sample_true.shape)\n",
        "  # print(x_sample_true.shape)\n",
        "  # print(y_true.shape)\n",
        "  # print(y_sample_fake.shape)\n",
        "  # print(x_sample_fake.shape)\n",
        "  # print(y_hat_fake.shape)\n",
        "\n",
        "\n",
        "  real = torch.cat((y_sample_true, x_sample_true, y_true), dim=2)\n",
        "  fake = torch.cat((y_sample_fake, x_sample_fake, y_hat_fake), dim=2)\n",
        "  # print(fake.shape)\n",
        "  grad_penalty2 = 0\n",
        "  for delta in np.random.uniform(0, 1, 30):\n",
        "        # Linearly interpolate between real and fake samples\n",
        "    interpolated = delta * real + (1 - delta) * fake\n",
        "        #print(interpolated[:,0:-2].shape)\n",
        "        # Calculate gradients of probabilities with respect to examples\n",
        "    interpolated.requires_grad_(True)\n",
        "        #print(\"interpolated: \", interpolated.size())\n",
        "        #print(\"mu: \", interpolated[:,:,-2].size())\n",
        "        #print(\"sigma: \", interpolated[:,:,-1].size())\n",
        "    # print(interpolated.shape)\n",
        "    # print(interpolated[:,:,0:10].shape)\n",
        "    # print(interpolated[:,:,10:20].shape)\n",
        "    # print(interpolated[:,:,20::].shape)\n",
        "    prob_interpolated = D(interpolated[:,:,0:10], interpolated[:,:,10:20], interpolated[:,:,20::])\n",
        "    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                                         grad_outputs=torch.ones_like(prob_interpolated),\n",
        "                                         create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        # Calculate gradient penalty\n",
        "    grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    grad_penalty2 += grad_penalty\n",
        "\n",
        "    # Return the average gradient penalty\n",
        "  return grad_penalty2 / 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtUI_dptJtN1"
      },
      "outputs": [],
      "source": [
        "D = Discriminator().to(device)\n",
        "G = Generator().to(device)\n",
        "D_optimizer = torch.optim.AdamW(D.parameters(), betas=(0.5, 0.9), lr=.0001)# use `adam`\n",
        "G_optimizer = torch.optim.AdamW(G.parameters(), betas=(0.5, 0.9), lr=.0001)# use `adam` #betas=(0.5, 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q11xJj0SLA_S"
      },
      "outputs": [],
      "source": [
        "D_losses = []\n",
        "G_losses = []\n",
        "G_losses_test = []\n",
        "Wasserstein_DS = []\n",
        "gradient_penaltys = []\n",
        "num_epochs = 600000\n",
        "batch_size = 20000\n",
        "min_val_loss = float(\"inf\")\n",
        "for epoch in range(num_epochs):\n",
        "  #print(epoch)\n",
        "  G.zero_grad()\n",
        "  D.zero_grad() # change the zero grad and move it before you call the D for the first time\n",
        "    #===========================================================================\n",
        "  z = torch.randn_like(parameters[idx_train, :,:])\n",
        "  D_loss = []\n",
        "  for i in range(2):\n",
        "    y_sample_true = resampled_mu_noisy[idx_train, :, :]\n",
        "    x_sample_true = resampled_x[idx_train, :, :]\n",
        "    y_true = parameters[idx_train,:,:]\n",
        "\n",
        "    D_real_decision = D(y_sample_true,\n",
        "                        x_sample_true,\n",
        "                        y_true)\n",
        "    D_real_loss = (D_real_decision.mean())\n",
        "\n",
        "    y_sample_fake = resampled_mu_noisy[idx_train, :, :]#torch.randn_like(resampled_mu_noisy[idx_train, :, :])\n",
        "    x_sample_fake = resampled_x[idx_train, :, :]#torch.randn_like(resampled_x[idx_train, :, :])\n",
        "    y_hat_fake = G(y_sample_fake,\n",
        "                            x_sample_fake,\n",
        "                            torch.randn_like(z))\n",
        "    D_fake_decision = D(y_sample_fake, x_sample_fake, y_hat_fake.unsqueeze(1))\n",
        "    #print(\"done\")\n",
        "    D_fake_loss = (D_fake_decision.mean())\n",
        "\n",
        "    gradient_penalty = calculate_gradient_penalty(\n",
        "        y_sample_true, x_sample_true, y_true,\n",
        "        y_sample_fake, x_sample_fake, y_hat_fake.unsqueeze(1),\n",
        "        D\n",
        "        )\n",
        "\n",
        "    Wasserstein_D = D_real_loss - D_fake_loss\n",
        "    lamb = 100\n",
        "    D_loss = -Wasserstein_D + (lamb*gradient_penalty)\n",
        "    Wasserstein_DS.append(Wasserstein_D.item())\n",
        "    gradient_penaltys.append(lamb*gradient_penalty.item())\n",
        "    # Back propagation\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "  y_hat = G(y_sample_true, x_sample_true, z)\n",
        "  #print(\"input_data>>>D\")\n",
        "  D_fake_decision = D(y_sample_true, x_sample_true, y_hat.unsqueeze(1))\n",
        "  D_fake_loss = (D_fake_decision.mean())\n",
        "  lamb_G = -1\n",
        "  mf_loss = 100*magic_formula_loss(x_sample_true, parameters[idx_train, :, :], y_hat)\n",
        "  rc_loss = 100*range_condition_loss(y_hat[:, 0], y_hat[:, 1], y_hat[:, 2], y_hat[:, 3])\n",
        "  s_loss = 100*slope_loss(parameters[idx_train, 0, 0], parameters[idx_train, 0, 1], parameters[idx_train, 0, 2],\n",
        "                                              y_hat[:, 0], y_hat[:, 1], y_hat[:, 2])\n",
        "  MSELOSS = 100*nn.MSELoss()(y_hat, parameters[idx_train, 0 ,:])\n",
        "  D_fake_loss = lamb_G*D_fake_loss\n",
        "  G_loss = (D_fake_loss) + mf_loss + rc_loss + s_loss + MSELOSS# add loss\n",
        "\n",
        "  G_loss.backward()\n",
        "  G_optimizer.step()\n",
        "\n",
        "  D_losses.append(D_loss.item())\n",
        "  #print(D_losses)\n",
        "  G_losses.append(G_loss.item())\n",
        "\n",
        "  # Validation\n",
        "  x_sample_test = resampled_x[idx_val, :, :]\n",
        "  y_resample_test = resampled_mu_noisy[idx_val, :, :]\n",
        "  y_hat_test = G(y_resample_test,\n",
        "                 x_sample_test,\n",
        "                 .5*torch.ones_like(parameters[idx_val,:, :]))\n",
        "  D_fake_decision_test = D(y_resample_test, x_sample_test, y_hat_test.unsqueeze(1))\n",
        "  D_fake_loss_test = (D_fake_decision_test.mean())\n",
        "\n",
        "  mf_loss_test = 100*magic_formula_loss(x_sample_test, parameters[idx_val, :, :], y_hat_test)\n",
        "  rc_loss_test = 100*range_condition_loss(y_hat_test[:, 0], y_hat_test[:, 1], y_hat_test[:, 2], y_hat_test[:, 3])\n",
        "  s_loss_test = 100*slope_loss(parameters[idx_val, 0, 0], parameters[idx_val, 0, 1], parameters[idx_val, 0, 2],\n",
        "                                              y_hat_test[:, 0], y_hat_test[:, 1], y_hat_test[:, 2])\n",
        "  MSELOSS_test = 100*nn.MSELoss()(y_hat_test, parameters[idx_val, 0 ,:])\n",
        "  D_fake_loss_test = lamb_G*D_fake_loss_test\n",
        "  G_loss_test = (D_fake_loss_test) + mf_loss_test + rc_loss_test + s_loss_test + MSELOSS_test# add loss\n",
        "  G_losses_test.append(G_loss_test.item())\n",
        "  if (G_loss_test.item() < min_val_loss):\n",
        "    min_val_loss = G_loss_test.item()\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'G_state_dict': G.state_dict(),\n",
        "            'D_state_dict': D.state_dict(),\n",
        "            'G_optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            'D_optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            'G_loss': G_losses,\n",
        "            'D_loss': D_loss\n",
        "            }, \"/content/drive/MyDrive/ML_spring2024/best_model_conv4_2.pth\")\n",
        "\n",
        "  epoch_test = 3000\n",
        "  if (((epoch%epoch_test == 0))| (epoch == num_epochs)):\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(18,13))\n",
        "    plt.subplot(331)\n",
        "    # Plot D_losses on the first y-axis (left) and G_losses on the second y-axis (right)\n",
        "    plt.plot(D_losses, color='blue')\n",
        "    plt.ylabel('D_losses')\n",
        "\n",
        "    plt.subplot(332)\n",
        "    plt.plot(G_losses, color='red')\n",
        "    plt.ylabel('G_losses')\n",
        "\n",
        "    plt.subplot(333)\n",
        "    # Plot KDE plots of input data and predicted data\n",
        "    plt.bar([\"W\", \"MF\", \"RC\", \"SL\", \"MSE\"], (D_fake_loss.item(), mf_loss.item(), rc_loss.item(), s_loss.item(), MSELOSS.item()), color=['blue', 'orange', 'green', 'maroon', 'gold'])\n",
        "\n",
        "    plt.subplot(334)\n",
        "    x_values = y_hat_test[:,0].cpu().detach().numpy()\n",
        "    y_values = parameters[idx_val,0,0].cpu().detach().numpy()\n",
        "    plt.scatter(x_values, y_values, color = 'k')\n",
        "    plt.axline([4, 4], [12, 12], color = 'r')\n",
        "    # plt.axline([0, 0], [1, 1], color = 'r')\n",
        "    plt.xlabel(\"Predicted B\")\n",
        "    plt.ylabel(\"Actual B\")\n",
        "\n",
        "\n",
        "    plt.subplot(335)\n",
        "    plt.bar([\"W\", \"GP\"], (Wasserstein_DS[-1], gradient_penaltys[-1]), color=['blue', 'orange'])\n",
        "\n",
        "    plt.subplot(336)\n",
        "    plt.plot(G_losses_test, color='orange')\n",
        "    plt.ylabel('G_losses_val')\n",
        "\n",
        "\n",
        "    plt.subplot(337)\n",
        "    x_values = y_hat_test[:,1].cpu().detach().numpy()\n",
        "    y_values = parameters[idx_val,0,1].cpu().detach().numpy()\n",
        "    plt.scatter(x_values, y_values, color = 'k')\n",
        "    plt.axline([1, 1], [2, 2], color = 'r')\n",
        "    # plt.axline([0, 0], [1, 1], color = 'r')\n",
        "    plt.xlabel(\"Predicted C\")\n",
        "    plt.ylabel(\"Actual C\")\n",
        "\n",
        "    plt.subplot(338)\n",
        "    x_values = y_hat_test[:,2].cpu().detach().numpy()\n",
        "    y_values = parameters[idx_val,0,2].cpu().detach().numpy()\n",
        "    plt.scatter(x_values, y_values, color = 'k')\n",
        "    plt.axline([.1, .1], [1.9, 1.9], color = 'r')\n",
        "    # plt.axline([0, 0], [1, 1], color = 'r')\n",
        "    plt.xlabel(\"Predicted D\")\n",
        "    plt.ylabel(\"Actual D\")\n",
        "\n",
        "    plt.subplot(339)\n",
        "    x_values = y_hat_test[:,3].cpu().detach().numpy()\n",
        "    y_values = parameters[idx_val,0,3].cpu().detach().numpy()\n",
        "    plt.scatter(x_values, y_values, color = 'k')\n",
        "    plt.axline([-10, -10], [1, 1], color = 'r')\n",
        "    # plt.axline([0, 0], [1, 1], color = 'r')\n",
        "    plt.xlabel(\"Predicted E\")\n",
        "    plt.ylabel(\"Actual E\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdpL6SYe_fXM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gshMzCG6cRSG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}